{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Results of all Models for a given Data and Model Version\n",
    "\n",
    "Loop over all Folds and Models and save the results in a dataframe.  \n",
    "Calculate a threshold for outcome prediction on the validation data (based on geometric mean) and apply it to the test data.  \n",
    "Add a normalized prediction uncertainty based on the standard deviation of the predictions of the different models.  \n",
    "Check the calibrartion plots.\n",
    "\n",
    "## Load Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install statsmodels\n",
    "#!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(\"TF  Version\",tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check and set path before loading modules\n",
    "print(os.getcwd())\n",
    "DIR = \"/tf/notebooks/schnemau/xAI_stroke_3d/\"\n",
    "if os.getcwd() != DIR:\n",
    "    os.chdir(DIR)\n",
    "    print(os.getcwd())\n",
    "\n",
    "import functions_metrics as fm\n",
    "import functions_model_definition as md\n",
    "import functions_read_data as rdat\n",
    "\n",
    "#ontram functions\n",
    "from k_ontram_functions.ontram import ontram\n",
    "from k_ontram_functions.ontram_loss import ontram_loss\n",
    "from k_ontram_functions.ontram_metrics import ontram_acc, ontram_auc\n",
    "from k_ontram_functions.ontram_predict import predict_ontram, get_parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look at some training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots outside the loop\n",
    "plt.figure(figsize=(30, 10))\n",
    "ax1 = plt.subplot(1, 3, 1)\n",
    "ax2 = plt.subplot(1, 3, 2)\n",
    "last_activation = \"linear\" \n",
    "which_split = 0\n",
    "model_version = 1\n",
    "\n",
    "for i in range(5):\n",
    "    model_name = \"/tf/notebooks/schnemau/xAI_stroke_3d/weights/10Fold_CIB/hist_3d_cnn_binary_model_split\" + str(which_split) + \n",
    "                          \"_unnormalized_avg_layer_paper_model_\" + last_activation + \"_activation_\" + str(model_version) + \"_\" + str(i) + \".pkl\"\n",
    "    histplt = pkl.load(open(model_name, \"rb\")) \n",
    "    # Plot training history lines\n",
    "    ax1.plot(histplt['loss'], label=f\"loss_{i}\")\n",
    "    ax1.plot(histplt['val_loss'], label=f\"val_loss_{i}\")\n",
    "    ax2.plot(histplt['acc'], label=f\"acc_{i}\")\n",
    "    ax2.plot(histplt['val_acc'], label=f\"val_acc_{i}\")\n",
    "\n",
    "# Add legends and display the plot\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"10Fold_CIB\" \n",
    "layer_connection = \"globalAveragePooling\" \n",
    "last_activation = \"linear\" \n",
    "# Define Model Version\n",
    "model_version = 1\n",
    "\n",
    "# should csv be saved?\n",
    "save_file = False\n",
    "\n",
    "DATA_OUTPUT_DIR = DIR + \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the paths for the result assembly\n",
    "DATA_DIR = DIR + \"data/\" \n",
    "WEIGHT_DIR = DIR + \"weights/\" + version + \"/\"\n",
    "id_tab = pd.read_csv(DATA_DIR + \"10Fold_ids_V0.csv\", sep=\",\")\n",
    "X = np.load(DATA_DIR + \"prepocessed_dicom_3d.npy\")\n",
    "all_result_name = \"all_tab_results_\" + version + \"_M\" + str(model_version)\n",
    "which_splits = list(range(0,10)) # 10 Fold\n",
    "print(id_tab.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(WEIGHT_DIR)\n",
    "print(all_result_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = (128, 128, 28, 1)\n",
    "output_dim = 1\n",
    "batch_size = 6\n",
    "C = 2 \n",
    "\n",
    "model_3d = ontram(utils.img_model_linear_final(input_dim, output_dim))     \n",
    "\n",
    "model_3d.compile(optimizer=keras.optimizers.Adam(learning_rate=5*1e-5),\n",
    "                                loss=ontram_loss(C, batch_size),\n",
    "                                metrics=[ontram_acc(C, batch_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nrs = list(range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = [] # test dataset results\n",
    "valid_list = [] # validation datset results\n",
    "check_print = True\n",
    "\n",
    "# loop over splits and models\n",
    "start = time.time()\n",
    "for which_split in which_splits:\n",
    "    if check_print:\n",
    "        print(\" \")\n",
    "        print(\"---- Start Reading Data of Split \" + str(which_split) + \" ----\")\n",
    "        print(\" \")\n",
    "    \n",
    "\n",
    "    (X_train, X_valid, X_test), (y_train, y_valid, y_test) = rdat.split_data(id_tab, X, which_split)\n",
    "    X_train = np.expand_dims(X_train, axis=-1)\n",
    "    X_valid = np.expand_dims(X_valid, axis=-1)\n",
    "    X_test = np.expand_dims(X_test, axis=-1)\n",
    "    Y_train = to_categorical(y_train)\n",
    "    Y_valid = to_categorical(y_valid)\n",
    "    Y_test = to_categorical(y_test)\n",
    "\n",
    "\n",
    "    results = id_tab[id_tab[\"fold\" + str(which_split)] == \"test\"].copy()      \n",
    "        \n",
    "    # add variable with current split\n",
    "    results[\"test_split\"] = which_split        \n",
    "    \n",
    "    # create new df for validation set, to calculate classification threshold \n",
    "    # => not cheating when calc on valid\n",
    "    validation_results = pd.DataFrame(\n",
    "        {\"test_split\": which_split,\n",
    "         \"unfavorable\": y_valid}\n",
    "    )\n",
    "    #0 = favorable\n",
    "    #1 = unfavorable\n",
    "\n",
    "    if check_print:\n",
    "        print(\" \")\n",
    "        print(\"---- Starting Result Calculation of Split \" + str(which_split) + \" ----\")\n",
    "        print(\" \")\n",
    "        \n",
    "    y_test_preds = []\n",
    "    y_valid_preds = []\n",
    "    \n",
    "     \n",
    "    for model_nr in model_nrs:\n",
    "        if check_print:\n",
    "            print(\"Now calculating model nr. \" + str(model_nr))\n",
    "        \n",
    "        model_name = (\"3d_cnn_binary_model_split\" + str(which_split) + \n",
    "                          \"_unnormalized_avg_layer_paper_model_\" + last_activation + \"_activation_\" + str(model_version) + \"_\" + str(model_nr) + \".h5\")\n",
    "        \n",
    "        model_3d.load_weights(model_name)        \n",
    "         \n",
    "        #test\n",
    "        predic = 1-utils.sigmoid(model_3d.predict(X_test))\n",
    "        y_test_preds.append(predic.squeeze())\n",
    "        results[\"y_pred_model_\" + str(model_nr)] = y_test_preds[-1]\n",
    "\n",
    "        #valid\n",
    "        predicc = 1-utils.sigmoid(model_3d.predict(X_valid))\n",
    "        y_valid_preds.append(predicc.squeeze())\n",
    "        validation_results[\"y_pred_model_\" + str(model_nr)] = y_valid_preds[-1]   \n",
    "\n",
    "\n",
    "    y_test_preds = np.array(y_test_preds)\n",
    "    results[\"y_pred_linear_avg\"] = np.mean(y_test_preds, axis = 0)\n",
    "    results[\"y_pred_trafo_avg\"] = fm.sigmoid(np.mean(fm.inverse_sigmoid(y_test_preds), axis = 0))\n",
    "    \n",
    "    y_valid_preds = np.array(y_valid_preds)\n",
    "    validation_results[\"y_pred_linear_avg\"] = np.mean(y_valid_preds, axis = 0)\n",
    "    validation_results[\"y_pred_trafo_avg\"] = fm.sigmoid(np.mean(fm.inverse_sigmoid(y_valid_preds), axis = 0))\n",
    "        \n",
    "    results_list.append(results)\n",
    "    valid_list.append(validation_results)\n",
    "        \n",
    "if check_print:\n",
    "    end = time.time()\n",
    "    print(\" \")\n",
    "    print(\"---- DONE ----\")\n",
    "    print(\" \")   \n",
    "    print(\"Duration of Execution: \" + str(end-start))               "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Threshold\n",
    "\n",
    "Calculation of threshold for classification is done on validation data. Then applied to the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should threshold be calculated per split or over all?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "valid_thresholds = []\n",
    "\n",
    "for i, validation_results in enumerate(valid_list):\n",
    "    print(\" \")\n",
    "    print(\"---- Split \" + str(which_splits[i]) + \" ----\")\n",
    "    print(\" \")\n",
    "    \n",
    "    y_org = validation_results[\"unfavorable\"]\n",
    "    y_pred = validation_results[\"y_pred_trafo_avg\"]\n",
    "    \n",
    "    # calculate fpr, tpr and thresholds\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_org, y_pred)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    # calculate geometric mean of tpr and fpr to find best threshold\n",
    "    gmean = np.sqrt(tpr * (1 - fpr))\n",
    "\n",
    "    # Find the optimal threshold\n",
    "    index = np.argmax(gmean)\n",
    "    valid_thresholds.append(threshold[index])\n",
    "    print(\"Optimal Geometric Mean Threshold: \" + str(threshold[index]))\n",
    "    \n",
    "    # Calc Acc\n",
    "    y_pred_label = (y_pred >= threshold[index]).squeeze()\n",
    "    print(\"Accuracy to beat: \" + str(1 - np.mean(y_org)))\n",
    "    print(\"Accuracy: \" + str(np.mean(y_pred_label == y_org)))\n",
    "          \n",
    "    print(\"Spezifität: \", 1-fpr[index])\n",
    "    print(\"Sensitivität: \", tpr[index])\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (10,5))\n",
    "    \n",
    "    # method I: plt\n",
    "    ax1.title.set_text('Receiver Operating Characteristic')\n",
    "    ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    ax1.scatter(fpr[index], tpr[index], color = \"red\", s = 50)\n",
    "    ax1.legend(loc = 'lower right')\n",
    "    ax1.plot([0, 1], [0, 1],'r--')\n",
    "    ax1.set_xlim([0, 1])\n",
    "    ax1.set_ylim([0, 1])\n",
    "    ax1.set_ylabel('True Positive Rate')\n",
    "    ax1.set_xlabel('False Positive Rate')\n",
    "    \n",
    "    sns.boxplot(x = \"unfavorable\",\n",
    "        y = \"y_pred_trafo_avg\",\n",
    "        data = validation_results,\n",
    "        ax = ax2)\n",
    "    sns.stripplot(x = \"unfavorable\",\n",
    "        y = \"y_pred_trafo_avg\",\n",
    "        color = 'blue',\n",
    "        data = validation_results,\n",
    "        ax = ax2)\n",
    "    ax2.axhline(y = threshold[index], color = \"red\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_thresholds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Threshold to Testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i, results in enumerate(results_list):\n",
    "    print(\" \")\n",
    "    print(\"---- Split \" + str(which_splits[i]) + \" ----\")\n",
    "    print(\" \")\n",
    "\n",
    "    results[\"threshold\"] = valid_thresholds[i]\n",
    "    results[\"y_pred_class\"] = (results[\"y_pred_trafo_avg\"] >= results[\"threshold\"]).astype(int)\n",
    "    \n",
    "    y_org = results[\"unfavorable\"]\n",
    "    y_pred = results[\"y_pred_class\"]\n",
    "    \n",
    "    # calculate fpr, tpr and thresholds\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_org, y_pred)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "      \n",
    "    # Calc Acc\n",
    "    print(\"Accuracy to beat: \" + str(1 - np.mean(y_org)))\n",
    "    print(\"Accuracy: \" + str(np.mean(y_pred == y_org)))\n",
    "          \n",
    "    print(\"Spezifität: \", 1-fpr[1])\n",
    "    print(\"Sensitivität: \", tpr[1])\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (10,5))\n",
    "    \n",
    "    # method I: plt\n",
    "    ax1.title.set_text('Receiver Operating Characteristic')\n",
    "    ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    ax1.legend(loc = 'lower right')\n",
    "    ax1.plot([0, 1], [0, 1],'r--')\n",
    "    ax1.set_xlim([0, 1])\n",
    "    ax1.set_ylim([0, 1])\n",
    "    ax1.set_ylabel('True Positive Rate')\n",
    "    ax1.set_xlabel('False Positive Rate')\n",
    "    \n",
    "    sns.boxplot(x = \"unfavorable\",\n",
    "        y = \"y_pred_trafo_avg\",\n",
    "        data = results,\n",
    "        ax = ax2)\n",
    "    sns.stripplot(x = \"unfavorable\",\n",
    "        y = \"y_pred_trafo_avg\",\n",
    "        color = 'blue',\n",
    "        data = results,\n",
    "        ax = ax2)\n",
    "    ax2.axhline(y = valid_thresholds[i], color = \"red\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concat all Pandas and Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = pd.concat(results_list)\n",
    "all_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_org = all_results[\"unfavorable\"]\n",
    "y_pred = all_results[\"y_pred_class\"]\n",
    "\n",
    "# calculate fpr, tpr and thresholds\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_org, y_pred)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "# Calc Acc\n",
    "y_pred_label = all_results[\"y_pred_class\"]\n",
    "print(\"Accuracy to beat: \" + str(1 - np.mean(y_org)))\n",
    "print(\"Accuracy: \" + str(np.mean(y_pred_label == y_org)))\n",
    "\n",
    "print(\"Spezifität: \", 1-fpr[1])\n",
    "print(\"Sensitivität: \", tpr[1])\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (10,5))\n",
    "\n",
    "# method I: plt\n",
    "ax1.title.set_text('Receiver Operating Characteristic')\n",
    "ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "ax1.legend(loc = 'lower right')\n",
    "ax1.plot([0, 1], [0, 1],'r--')\n",
    "ax1.set_xlim([0, 1])\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "\n",
    "sns.boxplot(x = \"unfavorable\",\n",
    "    y = \"y_pred_trafo_avg\",\n",
    "    data = all_results,\n",
    "    ax = ax2)\n",
    "sns.stripplot(x = \"unfavorable\",\n",
    "    y = \"y_pred_trafo_avg\",\n",
    "    hue = 'y_pred_class',\n",
    "    palette=[\"C2\", \"C3\", \"k\"],\n",
    "    data = all_results,\n",
    "    alpha = 0.75,\n",
    "    ax = ax2)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation of one threshold\n",
    "\n",
    "1. Threshold based on all validation data. This makes not much sense as each fold has different models.\n",
    "2. Threshold based on the validation data of each fold. This approach is used in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.\n",
    "all_valid_results = pd.concat(valid_list)\n",
    "\n",
    "y_org = all_valid_results[\"unfavorable\"]\n",
    "y_pred = all_valid_results[\"y_pred_trafo_avg\"]\n",
    "\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_org, y_pred)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "# calculate geometric mean of tpr and fpr to find best threshold\n",
    "gmean = np.sqrt(tpr * (1 - fpr))\n",
    "\n",
    "# Find the optimal threshold\n",
    "index = np.argmax(gmean)\n",
    "valid_threshold = threshold[index]\n",
    "print(\"threshold: \", threshold[index])\n",
    "\n",
    "\n",
    "y_pred_label = (y_pred >= threshold[index]).squeeze()\n",
    "print(\"Accuracy to beat: \" + str(1 - np.mean(y_org)))\n",
    "print(\"Accuracy: \" + str(np.mean(y_pred_label == y_org)))\n",
    "\n",
    "print(\"Spezifität: \", 1-fpr[index])\n",
    "print(\"Sensitivität: \", tpr[index])\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (10,5))\n",
    "\n",
    "# method I: plt\n",
    "ax1.title.set_text('Receiver Operating Characteristic')\n",
    "ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "ax1.scatter(fpr[index], tpr[index], color = \"red\", s = 50)\n",
    "ax1.legend(loc = 'lower right')\n",
    "ax1.plot([0, 1], [0, 1],'r--')\n",
    "ax1.set_xlim([0, 1])\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "\n",
    "sns.boxplot(x = \"unfavorable\",\n",
    "    y = \"y_pred_trafo_avg\",\n",
    "    data = all_valid_results,\n",
    "    ax = ax2)\n",
    "sns.stripplot(x = \"unfavorable\",\n",
    "    y = \"y_pred_trafo_avg\",\n",
    "    color = 'blue',\n",
    "    data = all_valid_results,\n",
    "    ax = ax2)\n",
    "ax2.axhline(y = threshold[index], color = \"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.\n",
    "all_results[\"threshold2\"] = valid_threshold\n",
    "all_results[\"y_pred_class2\"] = (all_results[\"y_pred_trafo_avg\"] >= all_results[\"threshold2\"]).astype(int)\n",
    "\n",
    "y_org = all_results[\"unfavorable\"]\n",
    "y_pred = all_results[\"y_pred_class2\"]\n",
    "y_pred_prob = all_results[\"y_pred_trafo_avg\"]\n",
    "\n",
    "# calculate fpr and tpr for probabilities\n",
    "fpr_prob, tpr_prob, thresholds_prob = metrics.roc_curve(y_org, y_pred_prob)\n",
    "\n",
    "# calculate fpr, tpr and thresholds for class\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_org, y_pred)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "# calculate AUC confint\n",
    "AUC0_CI = fm.compute_auc_ci(y_org, y_pred_prob)\n",
    "\n",
    "# calculate spez and sens confint\n",
    "cm = confusion_matrix(y_org, y_pred)\n",
    "# sens\n",
    "nobs = sum([cm[1,0],cm[1,1]])\n",
    "count = sum([cm[1,1]])\n",
    "sens_ci_low, sens_ci_upp = proportion_confint(count , nobs,  alpha=0.05, method='wilson')\n",
    "#spec \n",
    "nobs = sum([cm[0,1],cm[0,0]])\n",
    "count = sum([cm[0,0]])\n",
    "spec_ci_low, spec_ci_upp = proportion_confint(count , nobs,  alpha=0.05, method='wilson')\n",
    "\n",
    "# Calc Acc\n",
    "print(\"Accuracy to beat:     \" + str(round(1 - np.mean(y_org), 4)))\n",
    "print(\"Accuracy:             \" + str(round(np.mean(y_pred == y_org), 4)))\n",
    "print(\"-----------------[95% Conf.]-----------------\")\n",
    "\n",
    "print(\"AUC of probabilities: \" + str(round(metrics.auc(fpr_prob, tpr_prob), 4)) + \" \" +\n",
    "                                 str(np.around([AUC0_CI[0], AUC0_CI[1]],4)))\n",
    "print(\"NLL :                 \" + str(round(metrics.log_loss(y_org, y_pred_prob), 4)))\n",
    "\n",
    "print(\"Threshold:            \" + str(round(valid_threshold, 4)))\n",
    "\n",
    "print(\"Spezifität:           \" + str(round(1-fpr[1], 4)) + \" \" +\n",
    "                                 str(np.around([spec_ci_low, spec_ci_upp],4)))\n",
    "print(\"Sensitivität:         \" + str(round(tpr[1], 4)) + \" \" +\n",
    "                                 str(np.around([sens_ci_low, sens_ci_upp],4)))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (10,5))\n",
    "\n",
    "# method I: plt\n",
    "ax1.title.set_text('Receiver Operating Characteristic')\n",
    "ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "ax1.legend(loc = 'lower right')\n",
    "ax1.plot([0, 1], [0, 1],'r--')\n",
    "ax1.set_xlim([0, 1])\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "\n",
    "sns.boxplot(x = \"unfavorable\",\n",
    "    y = \"y_pred_trafo_avg\",\n",
    "    data = all_results,\n",
    "    ax = ax2)\n",
    "sns.stripplot(x = \"unfavorable\",\n",
    "    y = \"y_pred_trafo_avg\",\n",
    "    hue = 'y_pred_class2',\n",
    "    palette=[\"C2\", \"C3\"],\n",
    "    data = all_results,\n",
    "    ax = ax2)\n",
    "plt.legend(title='predicted class', loc='upper center')\n",
    "ax2.set(xlabel='true class', ylabel='predicted probability (class 1)')\n",
    "ax2.axhline(y = valid_threshold, color = \"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 10 choosen patients which are also in andreas split 6 \n",
    "all_results[all_results.p_id.isin([27,35,125,297,299,319,460,481,483,529])\n",
    "            ][[\"p_id\", \"mrs\", \"unfavorable\", \"test_split\", \n",
    "               \"y_pred_model_\" + \"0\", \"y_pred_model_\" + \"1\", \n",
    "               \"y_pred_model_\" + \"2\", \"y_pred_model_\" + \"3\", \n",
    "               \"y_pred_model_\" + \"4\", \n",
    "               \"y_pred_trafo_avg\", \"y_pred_class\"]].sort_values(by = \"p_id\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Model Uncertainty\n",
    "\n",
    "Use the standard deviation of the predictions as a measure of uncertainty. Then use min max normalization to scale the uncertainty between 0 and 1.  \n",
    "Compare the uncertainty with the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results[\"y_pred_std\"] = all_results[[\"y_pred_model_\" + str(i) for i in range(5)]].std(axis = 1)\n",
    "all_results[\"y_pred_unc\"] = (all_results[\"y_pred_std\"] - all_results.y_pred_std.min()) / (\n",
    "    all_results.y_pred_std.max() - all_results.y_pred_std.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot uncertainty\n",
    "plt.figure(figsize = (4.6,5))\n",
    "sns.boxplot(x = \"unfavorable\",\n",
    "    y = \"y_pred_unc\",\n",
    "    data = all_results)\n",
    "g = sns.stripplot(x = \"unfavorable\",\n",
    "    y = \"y_pred_unc\",\n",
    "    hue = 'y_pred_class2',\n",
    "    alpha = 0.75,\n",
    "    palette=[\"C2\", \"C3\"],\n",
    "    data = all_results)\n",
    "plt.legend(title='predicted class', loc='upper center')\n",
    "g.set(xlabel='true class', ylabel='prediction uncertainty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results[\"pred_correct\"] = all_results[\"y_pred_class2\"] == all_results[\"unfavorable\"] \n",
    "\n",
    "sns.boxplot(x = \"pred_correct\",\n",
    "    y = \"y_pred_unc\",\n",
    "    data = all_results)\n",
    "sns.stripplot(x = \"pred_correct\",\n",
    "    y = \"y_pred_unc\",\n",
    "    hue = 'unfavorable',\n",
    "    palette=[\"C2\", \"C3\"],\n",
    "    data = all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(\n",
    "           x = \"y_pred_trafo_avg\",\n",
    "           y = \"y_pred_unc\",\n",
    "           hue = \"unfavorable\",\n",
    "           data = all_results)\n",
    "plt.axvline(x = valid_threshold, color = \"red\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results\n",
    "\n",
    "Delete threshold 1 because mulitple different thresholds does not make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = all_results.drop(columns = [\"y_pred_class\", \"threshold\"])\n",
    "all_results = all_results.rename(columns = {\"y_pred_class2\": \"y_pred_class\", \"threshold2\": \"threshold\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_file:\n",
    "    all_results.to_csv(DATA_OUTPUT_DIR + all_result_name + \".csv\",  index=False) # rename output file!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_result_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Splits\n",
    "\n",
    "Only used for andreas/paper split:  \n",
    "Shows the different predictions (and uncertainties) for patients which were in multiple test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if version == \"andrea\":    \n",
    "    andrea_splits = pd.read_csv(split_path, \n",
    "                                    sep='\\,', header = None, engine = 'python', \n",
    "                                    usecols = [1,2,3]).apply(lambda x: x.str.replace(r\"\\\"\",\"\"))\n",
    "    andrea_splits.columns = andrea_splits.iloc[0]\n",
    "    andrea_splits.drop(index=0, inplace=True)\n",
    "    andrea_splits = andrea_splits.astype({'idx': 'int32', 'spl': 'int32'})\n",
    "    print(andrea_splits[andrea_splits[\"type\"].isin([\"test\"])].idx.nunique())\n",
    "    print(sum(andrea_splits[\"type\"].isin([\"test\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if version == \"andrea\":\n",
    "    all_p_dup = all_results[all_results[\"p_id\"].duplicated()][\"p_id\"].unique()\n",
    "    all_dup = all_results[all_results[\"p_id\"].isin(all_p_dup)].sort_values(\"p_id\")\n",
    "    all_dup[\"p_id\"].value_counts().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if version == \"andrea\":\n",
    "    print(\"Total duplicate: \", len(all_dup[\"p_id\"].value_counts()))\n",
    "\n",
    "    print(\"gleich klassifieziert (%): \", np.mean(all_dup.groupby(\"p_id\").y_pred_class.mean().isin([0,1])))\n",
    "    print(\"gleich klassifieziert (abs): \", np.sum(all_dup.groupby(\"p_id\").y_pred_class.mean().isin([0,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if version == \"andrea\":\n",
    "    sns.histplot(all_dup.groupby(\"p_id\").y_pred_trafo_avg.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if version == \"andrea\":\n",
    "    all_dup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration Plot\n",
    "\n",
    "Plot the calibration of all models and the mean calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_plot_datas = []\n",
    "for split in which_splits:\n",
    "    dat = all_results.loc[all_results[\"test_split\"] == split]\n",
    "    cal_plot_datas.append(fm.cal_plot_data_prep(\n",
    "        dat[\"y_pred_trafo_avg\"].array, dat[\"unfavorable\"].array\n",
    "    ).select_dtypes(include=np.number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cal_plot_datas = pd.concat(cal_plot_datas)\n",
    "all_cal_plot_datas[\"iter\"] = all_cal_plot_datas.index\n",
    "all_cal_plot_datas = all_cal_plot_datas.groupby(\"iter\")[\n",
    "    [\"predicted_probability_middle\", \"observed_proportion\", \"observed_proportion_lower\", \"observed_proportion_upper\"]].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "figure(figsize=(5, 5), dpi=80)\n",
    "\n",
    "for i in range(len(which_splits)):\n",
    "    fm.cal_plot(cal_plot_datas[i], \n",
    "             \"predicted_probability_middle\", \"observed_proportion\",\n",
    "                        \"observed_proportion_lower\", \"observed_proportion_upper\", alpha = .35, show = False)\n",
    "fm.cal_plot(all_cal_plot_datas, \"predicted_probability_middle\", \"observed_proportion\",\n",
    "                        \"observed_proportion_lower\", \"observed_proportion_upper\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Volume index\n",
    "\n",
    "Check if X_test is the same when selected via p_idx as when directly selected.  \n",
    "This must be true to make sure the correct volume is accessed for the heatmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"/tf/notebooks/hezo/stroke_zurich/data/dicom_2d_192x192x3_clean_interpolated_18_02_2021_preprocessed2.h5\", \"r\") as h5:\n",
    "# both images are the same\n",
    "    X_in = h5[\"X\"][:]\n",
    "    Y_img = h5[\"Y_img\"][:]\n",
    "    Y_pat = h5[\"Y_pat\"][:]\n",
    "    pat = h5[\"pat\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "results.p_id.array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(pat == results.p_id.array[index]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im1 = X_test[index].astype(\"float64\")\n",
    "im2 = X_in[np.argwhere(pat == results.p_id.array[index]).squeeze()].astype(\"float64\")\n",
    "np.allclose(im1, im2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "ax1.imshow(im1[:,:,14])\n",
    "ax2.imshow(im2[:,:,14])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
